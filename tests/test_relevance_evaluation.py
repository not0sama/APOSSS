#!/usr/bin/env python3
"""
APOSSS Relevance Evaluation Test Suite
Tests relevance metrics (nDCG, MAP) and performance metrics for the search system.
"""

import unittest
import numpy as np
import time
import requests
import json
from typing import List, Dict, Any
from sklearn.metrics import ndcg_score

class APOSSSRelevanceEvaluation(unittest.TestCase):
    """
    Comprehensive relevance evaluation test suite for APOSSS system.
    Tests nDCG, MAP, and performance metrics.
    """
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment with ground truth data."""
        cls.base_url = "http://localhost:5000"
        cls.test_queries = [
            {
                "query": "machine learning algorithms for medical diagnosis",
                "expected_relevant_docs": ["doc1", "doc2", "doc3"],
                "relevance_scores": [3, 2, 1],  # Ground truth relevance (1-3 scale)
                "category": "research_papers"
            },
            {
                "query": "AI researchers at MIT",
                "expected_relevant_docs": ["expert1", "expert2"],
                "relevance_scores": [3, 2],
                "category": "experts"
            },
            {
                "query": "GPU clusters for machine learning",
                "expected_relevant_docs": ["equipment1", "equipment2"],
                "relevance_scores": [3, 2],
                "category": "equipment"
            },
            {
                "query": "research funding for AI projects",
                "expected_relevant_docs": ["funding1", "funding2", "funding3"],
                "relevance_scores": [3, 3, 2],
                "category": "funding"
            },
            {
                "query": "deep learning textbooks",
                "expected_relevant_docs": ["book1", "book2"],
                "relevance_scores": [3, 2],
                "category": "academic_library"
            }
        ]
        
        # Performance benchmarks
        cls.performance_targets = {
            "response_time_ms": 2000,  # 2 seconds max
            "throughput_qps": 10,      # 10 queries per second
            "precision_at_k": 0.7,     # 70% precision at top-k
            "recall_at_k": 0.6,        # 60% recall at top-k
            "ndcg_at_k": 0.65,         # 65% nDCG at top-k
            "map_score": 0.6           # 60% MAP score
        }
        
        cls.ranking_modes = ["traditional", "hybrid", "ltr_only"]
        cls.k_values = [1, 3, 5, 10]  # Top-k values for evaluation
        
    def setUp(self):
        """Set up for each test."""
        self.results = {}
        self.performance_metrics = {}
        self.relevance_metrics = {}
        
    def calculate_ndcg_at_k(self, relevance_scores: List[float], k: int) -> float:
        """Calculate nDCG@k for given relevance scores."""
        if not relevance_scores or k <= 0:
            return 0.0
        
        # Truncate to top-k
        relevance_scores = relevance_scores[:k]
        
        # Calculate DCG@k
        dcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(relevance_scores)])
        
        # Calculate ideal DCG@k
        ideal_relevance = sorted(relevance_scores, reverse=True)
        idcg = sum([(2**rel - 1) / np.log2(i + 2) for i, rel in enumerate(ideal_relevance)])
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def calculate_map_score(self, relevance_scores: List[float], threshold: float = 2.0) -> float:
        """Calculate Mean Average Precision (MAP) score."""
        if not relevance_scores:
            return 0.0
        
        relevant_positions = [i for i, score in enumerate(relevance_scores) if score >= threshold]
        
        if not relevant_positions:
            return 0.0
        
        precision_sum = 0.0
        for i, pos in enumerate(relevant_positions):
            precision_at_pos = (i + 1) / (pos + 1)
            precision_sum += precision_at_pos
        
        return precision_sum / len(relevant_positions)
    
    def calculate_precision_at_k(self, relevance_scores: List[float], k: int, threshold: float = 2.0) -> float:
        """Calculate Precision@k."""
        if not relevance_scores or k <= 0:
            return 0.0
        
        top_k_scores = relevance_scores[:k]
        relevant_count = sum(1 for score in top_k_scores if score >= threshold)
        
        return relevant_count / len(top_k_scores)
    
    def calculate_recall_at_k(self, relevance_scores: List[float], total_relevant: int, k: int, threshold: float = 2.0) -> float:
        """Calculate Recall@k."""
        if not relevance_scores or k <= 0 or total_relevant <= 0:
            return 0.0
        
        top_k_scores = relevance_scores[:k]
        relevant_retrieved = sum(1 for score in top_k_scores if score >= threshold)
        
        return relevant_retrieved / total_relevant
    
    def perform_search(self, query: str, ranking_mode: str = "hybrid") -> Dict[str, Any]:
        """Perform search request and return results."""
        try:
            start_time = time.time()
            
            response = requests.post(
                f"{self.base_url}/api/search",
                json={
                    "query": query,
                    "ranking_mode": ranking_mode,
                    "limit": 20
                },
                timeout=10
            )
            
            end_time = time.time()
            response_time = (end_time - start_time) * 1000  # Convert to ms
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "response_time_ms": response_time,
                    "results": data.get("results", []),
                    "total_results": data.get("total_results", 0),
                    "metadata": data.get("search_metadata", {}),
                    "relevance_categories": data.get("relevance_categories", {})
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "response_time_ms": response_time
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time_ms": 0
            }
    
    def test_ndcg_evaluation(self):
        """Test nDCG scores across different ranking modes and k values."""
        print("\nðŸŽ¯ Testing nDCG Evaluation...")
        
        ndcg_results = {}
        
        for ranking_mode in self.ranking_modes:
            ndcg_results[ranking_mode] = {}
            
            for query_data in self.test_queries:
                query = query_data["query"]
                expected_relevance = query_data["relevance_scores"]
                
                # Perform search
                search_result = self.perform_search(query, ranking_mode)
                
                if search_result["success"]:
                    # Simulate relevance scores based on search results
                    # In real implementation, this would be based on ground truth
                    results = search_result["results"]
                    
                    # Mock relevance scores for demonstration
                    # In production, this would be based on expert annotations
                    relevance_scores = []
                    for i, result in enumerate(results[:10]):  # Top 10 results
                        if i < len(expected_relevance):
                            relevance_scores.append(expected_relevance[i])
                        else:
                            relevance_scores.append(1.0)  # Default low relevance
                    
                    # Calculate nDCG@k for different k values
                    for k in self.k_values:
                        ndcg_k = self.calculate_ndcg_at_k(relevance_scores, k)
                        
                        if k not in ndcg_results[ranking_mode]:
                            ndcg_results[ranking_mode][k] = []
                        
                        ndcg_results[ranking_mode][k].append(ndcg_k)
                        
                        print(f"   ðŸ“Š {ranking_mode} - {query[:30]}... - nDCG@{k}: {ndcg_k:.3f}")
        
        # Calculate average nDCG scores
        avg_ndcg_results = {}
        for ranking_mode in self.ranking_modes:
            avg_ndcg_results[ranking_mode] = {}
            for k in self.k_values:
                if k in ndcg_results[ranking_mode]:
                    scores = ndcg_results[ranking_mode][k]
                    avg_ndcg_results[ranking_mode][k] = sum(scores) / len(scores) if scores else 0.0
                    
                    print(f"   âœ… Average nDCG@{k} for {ranking_mode}: {avg_ndcg_results[ranking_mode][k]:.3f}")
        
        # Store results for reporting
        self.relevance_metrics["ndcg"] = avg_ndcg_results
        
        # Assert minimum nDCG performance
        for ranking_mode in self.ranking_modes:
            for k in [1, 3, 5]:
                if k in avg_ndcg_results[ranking_mode]:
                    ndcg_score = avg_ndcg_results[ranking_mode][k]
                    self.assertGreaterEqual(
                        ndcg_score, 
                        self.performance_targets["ndcg_at_k"] * 0.8,  # 80% of target
                        f"nDCG@{k} too low for {ranking_mode}: {ndcg_score:.3f}"
                    )
    
    def test_map_evaluation(self):
        """Test MAP (Mean Average Precision) scores across ranking modes."""
        print("\nðŸ“ Testing MAP Evaluation...")
        
        map_results = {}
        
        for ranking_mode in self.ranking_modes:
            map_scores = []
            
            for query_data in self.test_queries:
                query = query_data["query"]
                expected_relevance = query_data["relevance_scores"]
                
                # Perform search
                search_result = self.perform_search(query, ranking_mode)
                
                if search_result["success"]:
                    results = search_result["results"]
                    
                    # Mock relevance scores for demonstration
                    relevance_scores = []
                    for i, result in enumerate(results[:10]):
                        if i < len(expected_relevance):
                            relevance_scores.append(expected_relevance[i])
                        else:
                            relevance_scores.append(1.0)
                    
                    # Calculate MAP score
                    map_score = self.calculate_map_score(relevance_scores)
                    map_scores.append(map_score)
                    
                    print(f"   ðŸ“Š {ranking_mode} - {query[:30]}... - MAP: {map_score:.3f}")
            
            # Calculate average MAP
            avg_map = sum(map_scores) / len(map_scores) if map_scores else 0.0
            map_results[ranking_mode] = avg_map
            
            print(f"   âœ… Average MAP for {ranking_mode}: {avg_map:.3f}")
        
        # Store results for reporting
        self.relevance_metrics["map"] = map_results
        
        # Assert minimum MAP performance
        for ranking_mode in self.ranking_modes:
            map_score = map_results[ranking_mode]
            self.assertGreaterEqual(
                map_score,
                self.performance_targets["map_score"] * 0.8,  # 80% of target
                f"MAP too low for {ranking_mode}: {map_score:.3f}"
            )
    
    def test_precision_recall_evaluation(self):
        """Test Precision@k and Recall@k metrics."""
        print("\nðŸŽ¯ Testing Precision@k and Recall@k...")
        
        precision_results = {}
        recall_results = {}
        
        for ranking_mode in self.ranking_modes:
            precision_results[ranking_mode] = {}
            recall_results[ranking_mode] = {}
            
            for k in self.k_values:
                precision_scores = []
                recall_scores = []
                
                for query_data in self.test_queries:
                    query = query_data["query"]
                    expected_relevance = query_data["relevance_scores"]
                    total_relevant = len([score for score in expected_relevance if score >= 2.0])
                    
                    # Perform search
                    search_result = self.perform_search(query, ranking_mode)
                    
                    if search_result["success"]:
                        results = search_result["results"]
                        
                        # Mock relevance scores
                        relevance_scores = []
                        for i, result in enumerate(results[:10]):
                            if i < len(expected_relevance):
                                relevance_scores.append(expected_relevance[i])
                            else:
                                relevance_scores.append(1.0)
                        
                        # Calculate Precision@k and Recall@k
                        precision_k = self.calculate_precision_at_k(relevance_scores, k)
                        recall_k = self.calculate_recall_at_k(relevance_scores, total_relevant, k)
                        
                        precision_scores.append(precision_k)
                        recall_scores.append(recall_k)
                
                # Calculate averages
                avg_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0.0
                avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0
                
                precision_results[ranking_mode][k] = avg_precision
                recall_results[ranking_mode][k] = avg_recall
                
                print(f"   ðŸ“Š {ranking_mode} - Precision@{k}: {avg_precision:.3f}, Recall@{k}: {avg_recall:.3f}")
        
        # Store results for reporting
        self.relevance_metrics["precision"] = precision_results
        self.relevance_metrics["recall"] = recall_results
        
        # Assert minimum performance
        for ranking_mode in self.ranking_modes:
            for k in [1, 3, 5]:
                if k in precision_results[ranking_mode]:
                    precision = precision_results[ranking_mode][k]
                    recall = recall_results[ranking_mode][k]
                    
                    self.assertGreaterEqual(
                        precision,
                        self.performance_targets["precision_at_k"] * 0.7,  # 70% of target
                        f"Precision@{k} too low for {ranking_mode}: {precision:.3f}"
                    )
                    
                    self.assertGreaterEqual(
                        recall,
                        self.performance_targets["recall_at_k"] * 0.7,  # 70% of target
                        f"Recall@{k} too low for {ranking_mode}: {recall:.3f}"
                    )
    
    def test_response_time_performance(self):
        """Test response time performance across different query types."""
        print("\nâš¡ Testing Response Time Performance...")
        
        response_times = {}
        
        for ranking_mode in self.ranking_modes:
            times = []
            
            for query_data in self.test_queries:
                query = query_data["query"]
                
                # Perform multiple searches for average
                for _ in range(3):
                    search_result = self.perform_search(query, ranking_mode)
                    
                    if search_result["success"]:
                        times.append(search_result["response_time_ms"])
                        
                        print(f"   â±ï¸ {ranking_mode} - {query[:30]}... - {search_result['response_time_ms']:.2f}ms")
            
            # Calculate statistics
            if times:
                avg_time = sum(times) / len(times)
                min_time = min(times)
                max_time = max(times)
                percentile_95 = np.percentile(times, 95)
                
                response_times[ranking_mode] = {
                    "average": avg_time,
                    "min": min_time,
                    "max": max_time,
                    "p95": percentile_95
                }
                
                print(f"   âœ… {ranking_mode} Performance: avg={avg_time:.2f}ms, min={min_time:.2f}ms, max={max_time:.2f}ms, p95={percentile_95:.2f}ms")
        
        # Store results for reporting
        self.performance_metrics["response_time"] = response_times
        
        # Assert performance targets
        for ranking_mode in self.ranking_modes:
            if ranking_mode in response_times:
                avg_time = response_times[ranking_mode]["average"]
                p95_time = response_times[ranking_mode]["p95"]
                
                self.assertLess(
                    avg_time,
                    self.performance_targets["response_time_ms"],
                    f"Average response time too high for {ranking_mode}: {avg_time:.2f}ms"
                )
                
                self.assertLess(
                    p95_time,
                    self.performance_targets["response_time_ms"] * 1.5,  # 150% of target for p95
                    f"95th percentile response time too high for {ranking_mode}: {p95_time:.2f}ms"
                )
    
    def test_throughput_performance(self):
        """Test system throughput (queries per second)."""
        print("\nðŸš€ Testing Throughput Performance...")
        
        throughput_results = {}
        
        for ranking_mode in self.ranking_modes:
            # Test with concurrent requests
            query = "machine learning algorithms"
            num_requests = 20
            
            start_time = time.time()
            successful_requests = 0
            
            for i in range(num_requests):
                search_result = self.perform_search(query, ranking_mode)
                if search_result["success"]:
                    successful_requests += 1
            
            end_time = time.time()
            total_time = end_time - start_time
            
            if total_time > 0:
                throughput = successful_requests / total_time
                throughput_results[ranking_mode] = {
                    "qps": throughput,
                    "successful_requests": successful_requests,
                    "total_requests": num_requests,
                    "total_time": total_time
                }
                
                print(f"   ðŸš€ {ranking_mode} Throughput: {throughput:.2f} QPS ({successful_requests}/{num_requests} successful)")
        
        # Store results for reporting
        self.performance_metrics["throughput"] = throughput_results
        
        # Assert throughput targets
        for ranking_mode in self.ranking_modes:
            if ranking_mode in throughput_results:
                qps = throughput_results[ranking_mode]["qps"]
                
                self.assertGreaterEqual(
                    qps,
                    self.performance_targets["throughput_qps"] * 0.5,  # 50% of target
                    f"Throughput too low for {ranking_mode}: {qps:.2f} QPS"
                )
    
    def test_generate_evaluation_report(self):
        """Generate comprehensive evaluation report."""
        print("\nðŸ“Š Generating Comprehensive Evaluation Report...")
        
        # Run all evaluations first
        self.test_ndcg_evaluation()
        self.test_map_evaluation()
        self.test_precision_recall_evaluation()
        self.test_response_time_performance()
        self.test_throughput_performance()
        
        # Generate report
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "relevance_metrics": self.relevance_metrics,
            "performance_metrics": self.performance_metrics,
            "performance_targets": self.performance_targets,
            "summary": {
                "ranking_modes_tested": self.ranking_modes,
                "queries_tested": len(self.test_queries),
                "k_values_tested": self.k_values
            }
        }
        
        # Save report to file
        with open("tests/relevance_evaluation_report.json", "w") as f:
            json.dump(report, f, indent=2)
        
        print("   âœ… Evaluation report saved to tests/relevance_evaluation_report.json")
        
        # Print summary
        print("\nðŸ“‹ EVALUATION SUMMARY:")
        print("=" * 50)
        
        if "ndcg" in self.relevance_metrics:
            print("nDCG Scores:")
            for mode in self.ranking_modes:
                if mode in self.relevance_metrics["ndcg"]:
                    ndcg_5 = self.relevance_metrics["ndcg"][mode].get(5, 0)
                    print(f"  {mode}: nDCG@5 = {ndcg_5:.3f}")
        
        if "map" in self.relevance_metrics:
            print("\nMAP Scores:")
            for mode in self.ranking_modes:
                if mode in self.relevance_metrics["map"]:
                    map_score = self.relevance_metrics["map"][mode]
                    print(f"  {mode}: MAP = {map_score:.3f}")
        
        if "response_time" in self.performance_metrics:
            print("\nResponse Time Performance:")
            for mode in self.ranking_modes:
                if mode in self.performance_metrics["response_time"]:
                    avg_time = self.performance_metrics["response_time"][mode]["average"]
                    print(f"  {mode}: avg = {avg_time:.2f}ms")
        
        print("=" * 50)

if __name__ == "__main__":
    unittest.main(verbosity=2) 